---
title: "Tarea Individual II"
subtitle: "Ciencia de Datos con R"
format: html
editor: visual
---

```{r set up, include=FALSE}
'echo = TRUE' 
'eval = FALSE' 
'message = FALSE' 
'warning = FALSE' 
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, eval = FALSE)

```

## Introducción

El objetivo de esta tarea es aplicar técnicas de **aprendizaje supervisado** utilizando `tidymodels`. Se plantean dos problemas: uno de **regresión** y otro de **clasificación**, utilizando modelos de **árbol de decisión** y **random forest**. Además, se debe interpretar y comunicar los resultados obtenidos.

## Parte 1 – Regresión

**Dataset:** `ames` (paquete `{modeldata}`)\
**Variable objetivo:** `Sale_Price` (precio de venta de la vivienda)

### 1.1 Preparación de datos

1.1.1. Cargar el dataset `ames` y convertirlo en tibble. Guardar como `ames_data`.

```{r}
#| label: ejercicio_111

data(ames)
ames_data <- as_tibble(ames)
```

1.1.2. Utilizar `Sale_Price` como variable respuesta. Guarda la formula como `formula_ames`.

```{r}
#| label: ejercicio_112
formula_ames <- Sale_Price ~ .
```

1.1.3. Dividir los datos en entrenamiento (80%) y test (20%). Guarda los conjuntos como `ames_train` y `ames_test`.

```{r}
#| label: ejercicio_113
set.seed(123) #reproducibilidad
ames_split <- initial_split(ames_data, prop = 0.80, strata = Sale_Price)
ames_test <- testing(ames_split)
ames_train <- training(ames_split)
```

### 1.2 Entrenamiento del modelo

1.2.1. Definir un modelo de árbol de decisión. Guardar como `tree_ames`.

```{r}
#| label: ejercicio_121
tree_ames <- decision_tree() %>%
  set_engine("rpart") %>%  
    set_mode("regression")
```

1.2.2. Entrenar el modelo utilizando `fit()`. Guardar el modelo entrenado como `fit_tree_ames`.

```{r}
#| label: ejercicio_122
fit_tree_ames <- tree_ames %>% 
  fit(formula_ames, data = ames_train)  
```

### 1.3 Evaluación del modelo

1.3.1. Predecir sobre el conjunto de test. Guardar las predicciones como `predictions_ames`.

```{r}
#| label: ejercicio_131

predictions_ames <- predict(fit_tree_ames, new_data = ames_test)%>%
  bind_cols(ames_test %>% select( Sale_Price)) 

```

1.3.2. Calcular **RMSE** y **R²** tanto para train y test. En el caso de `ames_train` guardar como `metrics_train_ames` y en el caso de `ames_test` como `metrics_test_ames`.

```{r}
#| label: ejercicio_132
predictions_train_ames <- predict(fit_tree_ames, new_data = ames_train) %>%
  bind_cols(ames_train %>% select(Sale_Price))
metrics_train_ames <- metric_set(rmse, rsq)(predictions_train_ames, truth = Sale_Price, estimate = .pred)
metrics_test_ames <- metric_set(rmse, rsq)(predictions_ames, truth = Sale_Price, estimate = .pred)
print(metrics_test_ames)
print(metrics_train_ames)
```

1.3.3. Graficar valores reales vs. estimados (`Sale_Price`) con línea de identidad, utilizando los datos de entrenamiento. Mostrar el gráfico y guardarlo como `plot_tree_ames`.

```{r}
#| label: ejercicio_133
plot_ames_preds <- ggplot(predictions_ames, aes(x = Sale_Price, y = .pred)) +
  geom_point(alpha = 0.5) +
  geom_abline( color = "darkgreen") +
  labs(title = "Precio Real comparado con el Precio Estimado",
       x = "Precio de Venta real",
       y = "Precio de Venta estimado") 
print(plot_ames_preds)
```

1.3.4. Interpretar las métricas y el gráfico. (En este caso, no es necesario un chunk de código, pero sí una breve explicación en el texto).

1.3.5. Evaluar si hay sobreajuste o subajuste. Esta respuesta es libre y puede realizar el códogo que considere necesario para evaluar el modelo.

Respuesta 1.3.4 y 1.3.5:

El error cuadratico medio mide el promedio de los errores de las predicciones del modelo. Si este mismo nos da un valor bajo inidca que la predicciones del modelo esta cerca de los valores reales, diciendonos que el modelo tiene un buen rendimiento.

El rsq mide que tan bienel modelo explica la variabilidad en los datos, un rsq cercano al uno nos quiere decir que la variabiliadad en los datos puede ser explicada por las variables que usamos en el modelo.

El error cuadratico medio es mas bajo en el conjunto de entrenamiento (37200) que en el conjunto de test (44135). Esto significa que el modelo comete un error promedio menor cuando predice sobre los datos que ya ha entrenado con ellos que con los datos de prueba. Es algo muy esperable que suceda y la diferencia no es tanta como para poder nombrar un sobreajuste severo.

Esto nos puede decir que el modelo tiene un poco sobreajuste, tomando los detalles y ruidos de los datos de entrenamiento perdiendo algo generalidad.

El rsq es más alto en el conjunto de entrenamiento (0.774) que en el conjunto de test (0.747). Esto indica que el modelo explica una mayor proporcion de la varianza en los precios de entrenamiento en comparación con los precios de test. Aunque la diferencia sea minima. Aqui podriamos descartar la idea del sobreajuste.

En conclusion final diría que no hay una gran diferencia en ambos para poder afirmar que exista un sobreajuste.

**Sobre el grafico:** los puntos tienden a seguir la linea verde, quiere decir que los precios estimados se aproximan a los precios reales. Se puede observar que con precios bajos el modelo predice mejor ya que los puntos son menos dispersos y mas cercanos a la linea de igualdad. En cambio, al haber pocos precios de venta tan altos el precio estimado es menor con el verdadero precio de venta.

### 1.4 Interpretación del árbol

1.4.1. Visualizar el árbol. Guardar el gráfico como `plot_tree_ames_final`.

```{r}
#| label: ejercicio_141
plot_tree_ames_final <- rpart.plot( fit_tree_ames$fit, 
                                   roundint = FALSE) 


```

1.4.2. Interpretar brevemente las decisiones del árbol.

En el arbol de decisión para predecir el precio de las viviendas vemos que las variables mas influyentes son Gr_Liv_Area y las características del barrio "Neighborhood".

------------------------------------------------------------------------

## Parte 2 – Clasificación

**Dataset:** `attrition` (paquete `{modeldata}`)\
**Variable objetivo:** `Attrition` (abandono laboral: "Yes" o "No")

### 2.1 Preparación de datos

2.1.1. Cargar el dataset `attrition` y convertirlo en tibble. Guardar como `attrition_data`.

```{r}
#| label: ejercicio_211
data(attrition)
attrition_data <- as_tibble(attrition)
summary(attrition_data)
```

2.1.2. Asegurarse de que `Attrition` sea un factor. Sobreescribir la variable si es necesario.

```{r}
#| label: ejercicio_212
attrition_data <- attrition_data %>%  
  mutate(Attrition = factor(Attrition)) 
```

2.1.3. Dividir los datos en entrenamiento (80%) y test (20%) manteniendo la misma proporción de clases en ambos datasets. Guarda los conjuntos como `attrition_train` y `attrition_test`.

```{r}
#| label: ejercicio_213
attrition_split <- initial_split(attrition_data, prop = 0.80, strata = Attrition)
attrition_train <- training(attrition_split)
attrition_test <- testing(attrition_split)
```

### 2.2 Entrenamiento del modelo

2.2.1. Definir un modelo de random forest. Llevar a cabo la definición utilizando `rand_forest()` y `set_engine()`. Guardar como `rf_attrition`.

```{r}
#| label: ejercicio_221
rf_attrition <- rand_forest(trees = 1000) %>%
  set_engine("ranger", importance = "permutation") %>% #importance = "permutation" crucial para la parte 2.3.4
  set_mode("classification")  

```

2.2.2. Entrenar el modelo. Llevar a cabo el entrenamiento utilizando `fit()`. Utilizar la fórmula `Attrition ~ .` para incluir todas las variables predictoras. Guardar el modelo entrenado como `fit_rf_attrition`.

```{r}
#| label: ejercicio_222

fit_rf_attrition <- rf_attrition %>%
  fit(Attrition ~ ., data = attrition_train)
```

### 2.3 Evaluación del modelo

2.3.1. Predecir sobre el conjunto de test. Guardar las predicciones como `predictions_attrition`.

```{r}
#| label: ejercicio_231
predictions_attrition <- attrition_test %>%
  select(Attrition) %>% # Variable real (verdad)
  bind_cols(predict(fit_rf_attrition, attrition_test)) 
summary(predictions_attrition)
```

2.3.2. Calcular **accuracy**, **precision**, **recall** y **F1**. Guardar las métricas de entrenamiento como `metrics_train_attrition` y las de test como `metrics_test_attrition`.

```{r}
#| label: ejercicio_232
predictions_train_attrition <- attrition_train %>%
  select(Attrition) %>%
  bind_cols(predict(fit_rf_attrition, new_data = attrition_train))

metricas <- metric_set(accuracy, precision, recall, f_meas)
metrics_train_attrition<- predictions_train_attrition %>% metricas(truth = Attrition, estimate = .pred_class)
metrics_test_attrition <- predictions_attrition %>% metricas(truth = Attrition, estimate = .pred_class)
print(metrics_train_attrition)
print(metrics_test_attrition)

#el modelo muestra un buen rendimiento en el conjunto de test, por ejemplo identificando a los empleados que abandonan en un 100% de Recall. Se observa un sobreajuste moderado, debido que las metricas son mas altas en el conjunto de entrenamiento que en el de test.
```

2.3.3. Visualizar matriz de confusión con `autoplot(type = "heatmap")`. Imprimir el gráfico y guardarlo como `plot_confusion_attrition`.

```{r}
#| label: ejercicio_233
plot_confusion_attrition <- conf_mat(predictions_attrition, truth = Attrition, estimate = .pred_class) |>
autoplot(type = "heatmap") +
  scale_fill_gradient(low = "white", high = "red")
print(plot_confusion_attrition)
# se puede ver que predice de forma erronea los yes, debido a que solo predice 9(2 de forma erronea) y en realidad hay 48.A su vez vemos que tiene predispocion fuerte a que la predicion tome un "No".
```

2.3.4. Visualizar variables importantes con `vip()`. Imprimir el gráfico y guardarlo como `plot_vip_attrition`.

```{r}
#| label: ejercicio_234
plot_vip_attrition <- fit_rf_attrition$fit %>%
  vip(num_features = 10, fill = "darkgreen", color = "black", bar_width = 0.8) +
  labs(title = "Importancia de Variables (Random Forest)") +
  theme_minimal()
print(plot_vip_attrition)

#en el grafico vemos que las horas extras de trabajo (overtime) es la varibale que mas influye, con diferencia, en la predicion de abandono del trabajo. Nos esta diciendo que las horas extras son factor clave para entender el por qué los empleados abandonan el trabajo.

```

2.3.5. Interpretar las métricas y los errores. (En este caso, no es necesario un chunk de código, pero sí una breve explicación en el texto).

2.3.6. Evaluar si hay sobreajuste o subajuste. Esta respuesta es libre y puede realizar el código que considere necesario para evaluar el modelo.

Respondidas en el trasncurso de los ejercicios

------------------------------------------------------------------------

## Entrega

-   **La fecha limite de entrega es el 20 de junio de 2025.**

-   Las respuestas deben estar en este mismo archivo `.qmd`, el contenido deber ser completamente reproducible, es decir, cada `chunk` debe de funcionar sin errores para poder replicar los resultados.

-   No se aceptan archivos `.Rmd` o `.R` para la entrega. Solamente subir al repositorio el archivo `.qmd`con las respuestas.

-   Cada respuesta del ejercicio debe estar en el chunk correspondiente, no borrar la etiqueta del chunk `#| label: ejercicio_XX`.

-   Puede realizar pasos intermedios los que sean necesarios dentro del chunk pero debe de respetar el nombre del objeto final en el caso que se indique.

-   Los gráficos deben ser guardados en objetos y luego impresos en el caso que se indique que lo almacenen en un objeto. En el caso que no se indique, pueden ser impresos directamente.

-   Para comenzar la tarea deben de ir al siguiente link: GitHub Classroom. Una vez allí les va a pedir que indiquen su cuenta de GitHub y luego les va a crear un repositorio en su cuenta. Una vez creado el repositorio, deben de clonar el repositorio en su computadora y abrirlo con RStudio
